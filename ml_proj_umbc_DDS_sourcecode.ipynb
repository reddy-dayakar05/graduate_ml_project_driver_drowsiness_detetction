{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_37nN6wXdgXw"
      },
      "outputs": [],
      "source": [
        "!pip install dlib #package used in facial landmarks detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl8loQ92gg8R"
      },
      "outputs": [],
      "source": [
        "pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVUHwEoEBkIE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obDKWnl34bKs",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Dataset-1 pre-processing\n",
        "ht,wd =(128,128)\n",
        "batch_size = 32\n",
        "dataset1 = \"/content/drive/MyDrive/Colab Notebooks/DATASETS/Dataset-1\"\n",
        "\n",
        "# Splitting data into training data with 80% and testing data with 20%\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset1,\n",
        "    batch_size=32,\n",
        "    image_size=(ht,wd)\n",
        ")\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset1,\n",
        "    validation_split = 0.2,\n",
        "    subset= 'training',\n",
        "    seed=123,\n",
        "    label_mode='categorical',\n",
        "    image_size = (ht,wd),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset1,\n",
        "    validation_split = 0.2,\n",
        "    subset= 'validation',\n",
        "    seed=123,\n",
        "    label_mode='categorical',\n",
        "    image_size = (ht,wd),\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhFPU2T65uLu"
      },
      "outputs": [],
      "source": [
        "# CNN model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128,128, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "epochs=10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resnet50 model\n",
        "resnet_model = Sequential()\n",
        "pretrained_model = tf.keras.applications.ResNet50(include_top=False,\n",
        "                                                  input_shape=(128,128,3),\n",
        "                                                  pooling='avg',\n",
        "                                                  weights='imagenet')\n",
        "for layer in pretrained_model.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "resnet_model.add(pretrained_model)\n",
        "resnet_model.add(Flatten())\n",
        "resnet_model.add(Dense(512,activation='relu'))\n",
        "resnet_model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "resnet_model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "epochs=10"
      ],
      "metadata": {
        "id": "iovtupHHYCQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inception_Resnet model\n",
        "\n",
        "inception_resnet_model = Sequential()\n",
        "pretrained_model = tf.keras.applications.InceptionResNetV2(include_top=False,\n",
        "                                                  input_shape=(128,128,3),\n",
        "                                                  pooling='avg',\n",
        "                                                  weights='imagenet')\n",
        "for layer in pretrained_model.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "inception_resnet_model.add(pretrained_model)\n",
        "inception_resnet_model.add(Flatten())\n",
        "inception_resnet_model.add(Dense(512,activation='relu'))\n",
        "inception_resnet_model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "inception_resnet_model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "epochs=10"
      ],
      "metadata": {
        "id": "u0wIDdmsYiHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG16 model\n",
        "vgg16_model = Sequential()\n",
        "pretrained_model = tf.keras.applications.VGG16(include_top=False,\n",
        "                                                  input_shape=(128,128,3),\n",
        "                                                  pooling='avg',\n",
        "                                                  weights='imagenet')\n",
        "for layer in pretrained_model.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "vgg16_model.add(pretrained_model)\n",
        "vgg16_model.add(Flatten())\n",
        "vgg16_model.add(Dense(512,activation='relu'))\n",
        "vgg16_model.add(Dense(2,activation='softmax'))\n",
        "vgg16_model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "epochs=10"
      ],
      "metadata": {
        "id": "xH-4sgtDZZQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training CNN model with dataset-1\n",
        "cnn_history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "2rzHgLkQKyt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and loss graphs for CNN with dataset-1\n",
        "plt.plot(cnn_history.history['accuracy'], label='accuracy')\n",
        "plt.plot(cnn_history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1.5])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "#****************\n",
        "plt.plot(cnn_history.history['loss'], label='loss')\n",
        "plt.plot(cnn_history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim([0, 1.5])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g3Y8bJIkaKYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Resnet50 model with dataset-1\n",
        "resnet_history = resnet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "-6lFwqDja4Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and loss graphs for Resnet50 with dataset-1\n",
        "plt.plot(resnet_history.history['accuracy'], label='accuracy')\n",
        "plt.plot(resnet_history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1.5])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "#******************\n",
        "plt.plot(resnet_history.history['loss'], label='loss')\n",
        "plt.plot(resnet_history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 1.5])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ioPDOrFUa-_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Inception_Resnet model with dataset-1\n",
        "inception_resnet_history = inception_resnet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20,\n",
        ")"
      ],
      "metadata": {
        "id": "2RHsnK6vbRra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and loss graphs for Inception_Resnet with dataset-1\n",
        "plt.plot(inception_resnet_history.history['accuracy'], label='accuracy')\n",
        "plt.plot(inception_resnet_history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1.1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "#***************\n",
        "plt.plot(inception_resnet_history.history['loss'], label='loss')\n",
        "plt.plot(inception_resnet_history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 5])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QkIBn0j5bhcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training VGG16 model with dataset-1\n",
        "vgg16_history = vgg16_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "k6O6CIHhcIOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and loss graphs for VGG16 with dataset-1\n",
        "plt.plot(vgg16_history.history['accuracy'], label='accuracy')\n",
        "plt.plot(vgg16_history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1.1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "#*****************\n",
        "plt.plot(vgg16_history.history['loss'], label='loss')\n",
        "plt.plot(vgg16_history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 1.5])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r0pL_Jb6cdDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset-2 pre-processing\n",
        "ht,wd =(128, 128)\n",
        "batch_size = 32\n",
        "dataset2 = \"/content/drive/MyDrive/Colab Notebooks/DATASETS/Dataset-2\"\n",
        "\n",
        "# Splitting data into training data with 80% and testing data with 20%\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset2,\n",
        "    batch_size=32,\n",
        "    image_size=(ht,wd)\n",
        ")\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset2,\n",
        "    validation_split = 0.2,\n",
        "    subset= 'training',\n",
        "    seed=123,\n",
        "    label_mode='categorical',\n",
        "    image_size = (ht,wd),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset2,\n",
        "    validation_split = 0.2,\n",
        "    subset= 'validation',\n",
        "    seed=123,\n",
        "    label_mode='categorical',\n",
        "    image_size = (ht,wd),\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "7SaCW3iKc5d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training CNN model with dataset-2\n",
        "cnn_history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "bd5NCqWlRFvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and loss graphs for CNN with dataset-2\n",
        "plt.plot(cnn_history.history['accuracy'], label='accuracy')\n",
        "plt.plot(cnn_history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1.5])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "#****************\n",
        "plt.plot(cnn_history.history['loss'], label='loss')\n",
        "plt.plot(cnn_history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim([0, 1.5])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0WDT1eQRd_wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Resnet50 model with dataset-2\n",
        "resnet_history = resnet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "ptDOvhSFeKNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and loss graphs for Resnet50 with dataset-2\n",
        "plt.plot(resnet_history.history['accuracy'], label='accuracy')\n",
        "plt.plot(resnet_history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1.5])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "#******************\n",
        "plt.plot(resnet_history.history['loss'], label='loss')\n",
        "plt.plot(resnet_history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 1.5])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oZ2PMMyqeSPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Inception_Resnet model with dataset-2\n",
        "inception_resnet_history = inception_resnet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "EBo7BfCAeVCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and loss graphs for Inception_Resnet with dataset-2\n",
        "plt.plot(inception_resnet_history.history['accuracy'], label='accuracy')\n",
        "plt.plot(inception_resnet_history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1.1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "#***************\n",
        "plt.plot(inception_resnet_history.history['loss'], label='loss')\n",
        "plt.plot(inception_resnet_history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 1.5])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MG0j25sJejA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training VGG16 model with dataset-2\n",
        "vgg16_history = vgg16_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "nWBBZFhxeoky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and loss graphs for VGG16 with dataset-2\n",
        "plt.plot(vgg16_history.history['accuracy'], label='accuracy')\n",
        "plt.plot(vgg16_history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1.1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "#*****************\n",
        "plt.plot(vgg16_history.history['loss'], label='loss')\n",
        "plt.plot(vgg16_history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 1.5])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hEI_xVKIepe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAMPLE FOR PREDICTION USING CNN\n",
        "class_names = train_ds.class_names\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.io import imshow, imread\n",
        "og_image = cv2.imread('/content/drive/MyDrive/Colab Notebooks/DATASETS/Dataset-2/close/s0001_00048_0_0_0_0_0_01.png')\n",
        "resized  = cv2.resize(og_image,(64,64))\n",
        "og_image = np.expand_dims(resized,axis=0)\n",
        "pred= model.predict(og_image)\n",
        "oc =  class_names[np.argmax(pred)]\n",
        "print(\"predicted class is =  \",oc)"
      ],
      "metadata": {
        "id": "ZGbQaK7pxG-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracies comparison\n",
        "cnn_loss,cnn_acc = model.evaluate(val_ds)\n",
        "resnet_loss,resnet_acc = resnet_model.evaluate(val_ds)\n",
        "incepresnet_loss,incepresnet_acc = inception_resnet_model.evaluate(val_ds)\n",
        "vgg16_loss,vgg16_acc = vgg16_model.evaluate(val_ds)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the values and labels for the bar graph\n",
        "values = []\n",
        "values.append(cnn_acc)\n",
        "values.append(resnet_acc)\n",
        "values.append(incepresnet_acc)\n",
        "values.append(vgg16_acc)\n",
        "graph_labels = [\"CNN\", \"Resnet50\", \"InceptionResnet\", \"VGG16\"]\n",
        "\n",
        "# Create the bar graph\n",
        "plt.bar(graph_labels, values)\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.title(\"Comparison between accuracies of all models\")\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "# Display the bar graph\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vYRjhL4poNvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRK1PMKxFtMj"
      },
      "outputs": [],
      "source": [
        "# Dataset-3 - working of eye aspect ratio using video data\n",
        "import cv2\n",
        "import dlib\n",
        "from scipy.spatial import distance\n",
        "from imutils import face_utils\n",
        "from datetime import datetime\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Loading the pre-trained facial landmark predictor from Dlib library\n",
        "dlib_path = \"/content/drive/MyDrive/Colab Notebooks/DATASETS/shape_predictor_68_face_landmarks.dat\"\n",
        "detect = dlib.get_frontal_face_detector()\n",
        "predict = dlib.shape_predictor(dlib_path)\n",
        "\n",
        "# Define eye aspect ratio function\n",
        "def Eye_aspect_ratio(eye):\n",
        "    d1 = distance.euclidean(eye[1], eye[5]) + distance.euclidean(eye[2], eye[4])\n",
        "    d2 = distance.euclidean(eye[0], eye[3])\n",
        "    ear = d1 / (2.0 * d2)\n",
        "    return ear\n",
        "\n",
        "Ear_limit = 0.2\n",
        "Conseq = 20\n",
        "time = 2\n",
        "\n",
        "# Initialize variables\n",
        "f_counter = 0\n",
        "drowsy_frames = 0\n",
        "Eyes_closed_timer = None\n",
        "\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/Camera Roll/WIN_20231207_10_42_55_Pro.mp4\")\n",
        "\n",
        "while True:\n",
        "    r, frame = cap.read()\n",
        "    if not r:\n",
        "        break\n",
        "\n",
        "    # Convert frame into grayscale\n",
        "    g_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detect(g_image)\n",
        "\n",
        "    for face in faces:\n",
        "        # Detecting facial landmarks\n",
        "        f_landmarks = predict(g_image, face)\n",
        "        f_landmarks = [(lm.x, lm.y) for lm in f_landmarks.parts()]\n",
        "\n",
        "        # Extracting eye regions\n",
        "        left_eye = f_landmarks[36:42]\n",
        "        right_eye = f_landmarks[42:48]\n",
        "\n",
        "        left_eye = [(int(pt[0]), int(pt[1])) for pt in left_eye]\n",
        "        right_eye = [(int(pt[0]), int(pt[1])) for pt in right_eye]\n",
        "\n",
        "        # Calculate the eye aspect ratios\n",
        "        left_ear = Eye_aspect_ratio(left_eye)\n",
        "        right_ear = Eye_aspect_ratio(right_eye)\n",
        "        avg_ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        # Check for drowsiness\n",
        "        if avg_ear < Ear_limit:\n",
        "            f_counter += 1\n",
        "            if f_counter >= Conseq:\n",
        "                if Eyes_closed_timer is None:\n",
        "                    Eyes_closed_timer = datetime.now()\n",
        "                else:\n",
        "                    calculated_time = (datetime.now() - Eyes_closed_timer).total_seconds()\n",
        "                    if calculated_time >= time:\n",
        "                        print(\"Alert: Eyes closed for more than two seconds!\")\n",
        "            else:\n",
        "                Eyes_closed_timer = None\n",
        "        else:\n",
        "            f_counter = 0\n",
        "            Eyes_closed_timer = None\n",
        "\n",
        "    # Display the frame\n",
        "    cv2_imshow( frame)\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx0syVNvBoXW"
      },
      "outputs": [],
      "source": [
        "# Working of eye aspect ratio using a single facial image\n",
        "import os\n",
        "import cv2\n",
        "import dlib\n",
        "from scipy.spatial import distance\n",
        "from imutils import face_utils\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Directory containing your eye images\n",
        "image_dir = \"/content/drive/MyDrive/Camera Roll/WIN_20220329_21_06_07_Pro.jpg\"\n",
        "\n",
        "# Loop through each image in the directory\n",
        "for filename in os.listdir(image_dir):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "        frame = cv2.imread(os.path.join(image_dir, filename))\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = detect(gray)\n",
        "\n",
        "    for face in faces:\n",
        "        im = predict(gray, face)\n",
        "        im = face_utils.shape_to_np(im)\n",
        "\n",
        "        # Left and Right eye landmarks extraction\n",
        "        left_eye = im[42:48]\n",
        "        right_eye = im[36:42]\n",
        "\n",
        "        # Calculate the eye aspect ratio for both eyes\n",
        "        ear_left = Eye_aspect_ratio(left_eye)\n",
        "        ear_right = Eye_aspect_ratio(right_eye)\n",
        "\n",
        "        # Average the eye aspect ratios\n",
        "        ear_avg = (ear_left + ear_right) / 2.0\n",
        "\n",
        "        # Check if the EAR is below the Ear_limit and draw the points around eyes\n",
        "        if ear_avg < Ear_limit:\n",
        "            cv2.putText(frame, \"Drowsy\", (15, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "            for (x, y) in left_eye:\n",
        "              cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)\n",
        "            for (x, y) in right_eye:\n",
        "              cv2.circle(frame, (x, y), 2, (0, 0,255), -1)\n",
        "        else:\n",
        "            cv2.putText(frame, \"Awake\", (15, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "            for (x, y) in left_eye:\n",
        "              cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
        "            for (x, y) in right_eye:\n",
        "              cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Display the frame\n",
        "    cv2_imshow( frame)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}